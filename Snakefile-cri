"""
Snakefile specific for running assembly pipeline on Seattle Flu Study (SFS)
samples that automatically uploads resulting genomes to the SFS ID3C database.

To run:
    $ snakemake -k --snakefile Snakefile --configfile <config_file.json>
"""

import json
from urllib.parse import urljoin


# Include the main assembly pipeline
include: "Snakefile-base-cri"


# input function for the rule aggregate
def aggregate_input(wildcards):
    """
    Returns input for rule aggregate based on output from checkpoint align_rate.
    Set minimum align rate in config under "min_align_rate".
    """
    with open(checkpoints.mapped_reads.get(sample=wildcards.sample, reference=wildcards.reference).output[0]) as f:
        summary = json.load(f)
        all_segments_aligned = summary["all_segments_aligned"]
        min_reads = summary["minimum_reads_required"]
        mapped = summary["mapped_reads"]

        if not all_segments_aligned or mapped <= min_reads:
            return rules.not_mapped.output.not_mapped
        else:
            return rules.fasta_headers.output.masked_consensus


rule all:
    input:
        bamstats = expand("summary/bamstats/{reference}/{sample}.coverage_stats.txt", filtered_product,
               sample=all_ids,
               reference=all_references),
        consensus_genomes = expand("consensus_genomes/{reference}/{sample}.consensus.fasta", filtered_product,
            sample=all_ids,
            reference=all_references
        ),
        masked_consensus_genomes = expand("consensus_genomes/{reference}/{sample}.masked_consensus.fasta", filtered_product,
            sample=all_ids,
            reference=all_references
        ),
 

rule fasta_headers:
    input:
        masked_consensus = rules.vcf_to_consensus.output
    output:
        masked_consensus = "consensus_genomes/{reference}/{sample}.masked_consensus.fasta"
    shell:
        """
        cat {input.masked_consensus} | \
            perl -pi -e 's/(?<=>)[^>|]*(?<=|)/{wildcards.sample}/g' > \
            {output.masked_consensus}.temp
        awk '{{split(substr($0,2),a,"|"); \
            if(a[2]) print ">"a[1]"|"a[1]"-"a[2]"-"a[3]"|"a[2]"|"a[3]; \
            else print; }}' \
            {output.masked_consensus}.temp > {output.masked_consensus}
        rm {output.masked_consensus}.temp

        """


rule metadata_to_json:
    input:
        all_r1 = lambda wildcards: mapped[wildcards.sample][0],
        all_r2 = lambda wildcards: mapped[wildcards.sample][1]
    output:
        temp("consensus_genomes/{reference}/{sample}.metadata.json")
    shell:
        """
        python scripts/metadata_to_json.py {input.all_r1:q} {input.all_r2:q} > {output}
        """


rule masked_consensus_to_json:
    input:
        masked_consensus = rules.fasta_headers.output.masked_consensus
    output:
        temp("consensus_genomes/{reference}/{sample}.masked_consensus.json")
    shell:
        """
        python scripts/fasta_to_json.py {input.masked_consensus} > {output}
        """


rule summary_stats_to_json:
    input:
        bam_coverage = rules.bamstats.output.bamstats_file,
        bowtie2 = rules.map.output.bt2_log
    output:
        temp("consensus_genomes/{reference}/{sample}.summary_stats.json")
    shell:
        """
        python scripts/summary_stats_to_json.py --bamstats {input.bam_coverage} \
            --bowtie2 {input.bowtie2} > {output}
        """


rule aggregate:
    input:
        aggregate_input = aggregate_input
    output:
        aggregate_summary = "summary/aggregate/{reference}/{sample}.log"
    run:
        shell("echo 'Final output: {input.aggregate_input}' > {output}")
